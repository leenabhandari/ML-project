Python 2.7.14 (v2.7.14:84471935ed, Sep 16 2017, 20:19:30) [MSC v.1500 32 bit (Intel)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from pandas import read_csv
>>> import numpy as np
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn import linear_model
>>> from sklearn.model_selection import train_test_split
>>> from sklearn import tree
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> from sklearn.ensemble import RandomForestClassifier
>>> import string
>>> fname='h1b_sample3.csv'
>>> data=read_csv(fname)
>>> ds=data.values
>>> X_data1=ds[:,6]
>>> Y_data=ds[:,1]
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_data1, Y_data, test_size=0.10,random_state=7)
>>> model= RandomForestClassifier()
>>> model.fit(X_train.reshape(-1,1),Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test.reshape(-1,1),Y_test)
0.8066666666666666
>>> model= RandomForestClassifier(min_samples_leaf=5)
>>> model.fit(X_train.reshape(-1,1),Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=5, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test.reshape(-1,1),Y_test)
0.8377777777777777
>>> model=tree.DecisionTreeClassifier()
>>> model.fit(X_train.reshape(-1,1),Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test.reshape(-1,1),Y_test)
0.8166666666666667
>>> model=linear_model.LogisticRegression()
>>> model.fit(X_train.reshape(-1,1),Y_train)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
>>> model.score(X_test.reshape(-1,1),Y_test)
0.8422222222222222
>>> model=KNeighborsClassifier()
>>> model.fit(X_train.reshape(-1,1),Y_train)
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
>>> model.score(X_test.reshape(-1,1),Y_test)
0.8444444444444444
>>> from sklearn.svm import SVC
>>> model=SVC()
>>> model.fit(X_train.reshape(-1,1),Y_train)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
>>> model.score(X_test.reshape(-1,1),Y_test)
0.8333333333333334
>>> full_time=ds[:,5]
>>>
>>> for i in range(full_time.size):
...  if full_time[i]=='Y':
...   full_time[i]=1
...  else:
...   full_time[i]=0
...
>>> X_data2=np.c_[X_data1, full_time]
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_data2, Y_data, test_size=0.10,random_state=7)
>>> model= RandomForestClassifier()
>>> model.fit(X_train,Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test,Y_test)
0.82
>>> model=tree.DecisionTreeClassifier()
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.8166666666666667
>>> model = linear_model.LogisticRegression()
>>> model.fit(X_train,Y_train)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
>>> model.score(X_test,Y_test)
0.8422222222222222
>>> model=KNeighborsClassifier()
>>> model.fit(X_train,Y_train)
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
>>> model.score(X_test,Y_test)
0.8444444444444444
>>> model=SVC()
>>> model.fit(X_train,Y_train)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
>>> model.score(X_test,Y_test)
0.8333333333333334
>>> lon=ds[:,9]
>>> X_data3=np.c_[X_data2, lon]
>>> from sklearn.preprocessing import Imputer
>>> imp=Imputer()
>>> X_trans=imp.fit_transform(X_data3)
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_trans, Y_data, test_size=0.10,random_state=7)
>>> model=RandomForestClassifier()
>>> model.fit(X_train,Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test,Y_test)
0.8144444444444444
>>> model=tree.DecisionTreeClassifier()
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.7866666666666666
>>> model = linear_model.LogisticRegression()
>>> model.fit(X_train,Y_train)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
>>> model.score(X_test,Y_test)
0.8411111111111111
>>> model=KNeighborsClassifier()
>>> model.fit(X_train,Y_train)
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
>>> model.score(X_test,Y_test)
0.8444444444444444
>>> model=SVC()
>>> model.fit(X_train,Y_train)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
>>> model.score(X_test,Y_test)
0.8422222222222222
>>> lat=ds[:,10]
>>> X_data4=np.c_[X_data2, lat]
>>> X_trans1=imp.fit_transform(X_data4)
>>> X_trans=imp.fit_transform(X_data4)
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_trans, Y_data, test_size=0.10,random_state=7)
>>> model= RandomForestClassifier()
>>> model.fit(X_train,Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test,Y_test)
0.81
>>> model=tree.DecisionTreeClassifier()
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.7877777777777778
>>> soc=ds[:,3]
>>> for i in range(soc.size):
...   soc[i]=string.replace(soc[i],'MANAGERS','MANAGER')
...   soc[i]=string.replace(soc[i],'&','AND')
...   soc[i]=string.replace(soc[i],'FUNDRAISING','FUND RAISING')
...   soc[i]=string.replace(soc[i],'INFORMATON','INFORMATION')
...   soc[i]=string.replace(soc[i],'MANGERS','MANAGER')
...   soc[i]=string.replace(soc[i],'MANAGERE','MANAGER')
...
>>> sfit=np.unique(soc)
>>> vectorizer = CountVectorizer()
>>> vectorizer.fit_transform(sfit)
<13x24 sparse matrix of type '<type 'numpy.int64'>'
        with 41 stored elements in Compressed Sparse Row format>
>>> X_soc=vectorizer.transform(soc).toarray()
>>> X_trans1=np.c_[X_soc, X_trans]
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_trans1, Y_data, test_size=0.10,random_state=7)
>>> model= RandomForestClassifier()
>>> model.fit(X_train,Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test,Y_test)
0.8155555555555556
>>> model=tree.DecisionTreeClassifier()
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.7977777777777778
>>> ws=ds[:,8]
>>> c=np.array([])
>>> for i in range(ws.size):
...   a,b=ws[i].split(",")
...   c=np.append(c,b)
...
>>> workstate=np.unique(c)
>>> vec = CountVectorizer()
>>> vec.fit_transform(workstate)
<53x58 sparse matrix of type '<type 'numpy.int64'>'
        with 66 stored elements in Compressed Sparse Row format>
>>> X_ws=vec.transform(c).toarray()
>>> X_new=np.c_[X_data2, X_ws]
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_data, test_size=0.10,random_state=7)
>>> model
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.fit(X_train,Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test,Y_test)
0.8155555555555556
>>> model=tree.DecisionTreeClassifier()
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.8088888888888889
>>> model = linear_model.LogisticRegression()
>>> model.fit(X_train,Y_train)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
>>> model.score(X_test,Y_test)
0.8422222222222222
>>> model=KNeighborsClassifier()
>>> model.fit(X_train,Y_train)
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
>>> model.score(X_test,Y_test)
0.8455555555555555
>>> model=SVC()
>>> model.fit(X_train,Y_train)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
>>> model.score(X_test,Y_test)
0.8366666666666667
>>> X_new1=np.c_[X_new,X_soc]
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_new1, Y_data, test_size=0.10,random_state=7)
>>> model=tree.DecisionTreeClassifier()
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.8055555555555556
>>> model= RandomForestClassifier()
>>> model.fit(X_train,Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test,Y_test)
0.8144444444444444
>>> model = linear_model.LogisticRegression()
>>> model.fit(X_train,Y_train)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
>>> model.score(X_test,Y_test)
0.8422222222222222



>>> data=read_csv('h1b_kaggle1.csv')
>>> ds=data.values
>>> full_time=ds[:,5]
>>>
>>> for i in range(full_time.size):
...  if full_time[i]=='Y':
...   full_time[i]=1
...  else:
...   full_time[i]=0
...
...
>>> X_data1=ds[:,6]
>>> X_data2=np.c_[X_data1, full_time]
>>> Y_data=ds[:,1]
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_data2, Y_data, test_size=0.10,random_state=7)
>>> model=tree.DecisionTreeClassifier()
>>> imp=Imputer()
>>> X_trans=imp.fit_transform(X_data2)
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_trans, Y_data, test_size=0.10,random_state=7)
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.87046245566121
>>> model= RandomForestClassifier()
>>> model.fit(X_train,Y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
>>> model.score(X_test,Y_test)
0.8696198104881013

>>> model=tree.DecisionTreeClassifier(min_samples_leaf=50)
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_trans, Y_data, test_size=0.10,random_state=0)
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=50, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.8722676480873953
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_trans, Y_data, test_size=0.10,random_state=7)
>>> lon=ds[:,9]
>>> lat=ds[:,10]
>>> X_l=np.c_[lon,lat]
>>> X_trans1=imp.fit_transform(X_l)
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_trans1, Y_data, test_size=0.10,random_state=7)
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=50, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.8714982764076005
>>> X_new=np.c_[X_trans,X_trans1]
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_data, test_size=0.10,random_state=7)
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=50, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.8728505054205732
>>> X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_data, test_size=0.25,random_state=7)
>>> model.fit(X_train,Y_train)
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=50, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
>>> model.score(X_test,Y_test)
0.8720683921919713
